---
title: "W241 Final Report"
author: "David Hou, Ravi Ayyappan, Ahsen Qamar"
date: "April 2019"
output: 
  pdf_document: 
    keep_tex: yes
    fig_caption: yes
urlcolor: blue
linkcolor: blue
header-includes:
  \usepackage{float}
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = F, fig.pos = 'H')
library(data.table)
library(magrittr)
library(ggplot2)
library(stargazer)
```

## Introduction

The importance of good sleep habits is emphasized quite often in contemporary advice.  One aspect of good sleep hygiene is the avoidance of electronic devices with backlit screens prior to bed.  The light from these screens causes the brain to stay under the assumption of daytime, inhibiting the production of the sleep hormone melatonin.  In this experiment, we seek a quantitative measure of sleep quality degradation as a result of screen-time before bed.  Due to our limited expertise and resources for studying sleep science, we will perform the measaurement using a mobile app.

## Experiment Design

The app used is called [Sleep Cycle](www.sleepcycle.com) and utilizes the phone's microphone to measure a user's sleep cycles overnight.  It is primarily advertised for its alarm clock feature, which attempts to wake a user in the lightest phase of sleep in a preset time interval.  Each morning, the app displays a graph of the user's sleep cycles in the previous night and some relevant statistics.  Figure \ref{fig:example_night} shows an example of the output from one night's data.

```{r example_night, out.width='25%', fig.align='center', fig.cap='Example night'}
knitr::include_graphics('img/example_night.PNG')
```

One of these statistics is a "Sleep Quality" number, which is the principle outcome variable in this experiment.  According to the *Sleep Cycle* website, these four factors go into calculating Sleep Quality:

1. Amount of time spent in bed.
2. Amount of time spent in deep sleep.
3. Consistency of the sleep.
4. Amount of times where the app registered you as fully awake.

Of the four factors, users generally only have direct control over the first one.  In order to be less intrusive, we did not control the amount of time that users spend in bed, but we did instruct subjects to try to sleep at regular hours every night.  The other three factors are hopefully affected by screen-time, allowing us to measure a treatment effect.  The app also records a "snore" and "steps walked" value, but we have doubts about the accuracy of these measures and will not use them in our analysis.

The experiment took place over two weeks, from March 23 to April 5 of 2019.  Saturday was chosen as the start day of the week because it was feared that instructions to change sleep habits in the middle of the week might be ignored.  We recruited 35 people for the study using a Google survey, consisting of classmates, friends, and family.  In addition to asking for contact and demogrphic information, the survey also inquired about general sleep habits such as bedtime and pre-sleep activities.  While the survey did ask about electronic usage, the questions were mixed in among other questions such as workout habits and alcohol consumption.  Prior to commencement of the experiment, subjects were not made aware of the fact that screen-time was the critical variable of interest.

All communications to subjects occurred over email.  Every person in the same experimental group received identical emails (see Appendix).  When instructing subjects to use electronics prior to bed, we did not mandate any specific activity, as we felt it would increase non-complicance on an already-intrusive study.  We only specified that users should use some sort of eletronic device with a backlit screen for 30 minutes immediately before bedtime.  Similarly, we did not ask subjects in the other group to perform anything in lieu of elctronic usage.  These subjects were simply asked to refrain from using electronics with backlit screens 30 minutes before bedtime.

Subjects were assigned randomly to two groups, with group 1 receiving treatment (screen-time before bed) during the first week and group 2 receiving control (no screen-time before bed).  After the first week, subjects were instructed to swap to the opposite activity of what they were assigned to initially.  The swapping design allows us to perform within-subject comparisons while also leaving time for treatment effects to manifest themselves.  With the group that received treatment first, we are also able to perform some analysis on persistence effects.  Note that subjects were not informed of the instruction swap ahead of time, so we do not expect any anticipation effects.

The nature of experiment requires effort from subjects in both treatment and control.  That is, for people who normally use electronic devices before bed, the request to refrain from useage constitutes a significant change in pre-sleep ritual; for people who normally avoid electronics before bed, the opposite is true.  Therefore, we are concerned with two-sided non-compliance, which is especially difficult to measure in this experiment because people are unlikely to honestly report when they have deviated from our instructions.  To help combat this somewhat, we offered subjects some small monetary compensation and emphasized prioritization of honesty of compliance in reporting data.  Even so, we can never be sure of whether subjects followed instructions.

### Experiment Deviations

Between the closing the sign-up survey, we asked all respondents to begin using the app in order to establish baseline data and help the app calibrate (the website claims that the app becomes more accurate over time).  Sleep habits during this baseline usage were not closely controlled or monitored, as each subject signed up for the experiment at different dates.  However, we did ask for an export of this baseline data prior to starting the experiment, so that subjects had some awareness of the steps it takes to perform the export.  At this time, we learned that Android users could not perform the export due to missing functionality in the app.  As such, we asked Android users (4 out of the 38) to manually record the sleep times and quality from the app each day.

After closing the sign-up survey, we decided to also included ourselves as subjects in the experiment.  This was to slightly raise the number of subjects and out of curiosity for the effects on ourselves.  Though we were not aware of our own randomization a priori, we obviously had more knowledge of the experiment than other subjects, so we will analyze our own data with special care.

## Results

```{r}
df = data.table(read.csv('data/cleaned.csv', stringsAsFactors = F))
df = df %>%
    .[, treat := as.factor(treat)] %>%
    .[, start_date := as.Date(start_date)] %>%
    .[, end_date := as.Date(end_date)]
```

### Data Cleaning

We made as little modification to the raw data as possible, but a few exceptions exist.  We decided to omit entries for which the subject slept for a duration shorter than one hour.  On case by case basis, we determined that these entries were caused by improper usage of the app.  There were several entries where the user had two entries in one night, separated by some period of awakeness.  In these cases, one or both of the periods of sleep were also very short.  However, none of them fell under the one hour threshold for omission.

Of the 38 people who originally signed up for the study, only 29 people actually submitted final data by the time of writing this report.  As such, we excluded their results.  Note that these subjects are not necessarily attriters, as we could not collect data from treatment or control.  

### Analysis

Figure \ref{fig:quality_by_treatment_fig} shows the sleep quality, averaged over the respective time periods, for each treatment assignment.  As mentioned above, the baseline data was collected in the time between subjects signing up and the experiment officially starting.  We did not tightly control baseline data and not every subject provided it.

```{r quality_by_treatment_fig, fig.cap='\\label{fig:quality_by_treatment_fig} Mean sleep quality by treatment'}
# I'm going to be reusing df2 as a variable name a lot.  I'm too lazy to come up with something more meaningful.
df2 = df[, .(mean = mean(quality), sd = sd(quality)), by = treat]
df2$label = c('-1 (Baseline)', '0 (No Screen Time)', '1 (Screen Time)')
ggplot(df2, aes(x=label, y=mean)) + 
    geom_bar(stat="identity") +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2) +
    labs(x = NULL, y = 'Mean Quality', title = 'Mean Sleep Quality by Treatment')
```

It is already clear that our experiment is too underpowered to detect a treatment effect, as the error bars are extremely large comapred to the barely noticeable differences in means.  Table \ref{tab:quality_by_treatment_tab} shows the tabular results for Figure \ref{fig:quality_by_treatment_fig}.  The point estimate for screen time is slightly lower than that of no screen time, but the standard error is way too large for this difference to be statistically significant.

```{r quality_by_treatment_tab, results='asis'}
df2 = df2[, c(4,2,3)] # reorder columns
df2$label = c('Baseline', 'No Screen Time', 'Screen Time')
names(df2) = c('Label', 'Mean', 'St. Dev.')
stargazer(
    df2, 
    header = F,
    summary = F, 
    rownames = F,
    title = '\\label{tab:quality_by_treatment_tab} Mean sleep quality by treatment')
```

Figure \ref{fig:quality_by_date_group1_fig} and Figure \ref{fig:quality_by_date_group2_fig} show the mean sleep quality by dates for the two groups.  Recall that group 1 received treatment (screen time) in week 1 of the experiment and switched to control (no screen time) in week 2.  Group 2 had the opposite schedule.  Again, the large error bars make it hard to draw any sort of conclusion for whether the switch in treatment has any effect.

```{r quality_by_date_group1_fig, fig.cap='\\label{fig:quality_by_date_group1_fig} Mean sleep quality by date in group which received screen time first', warning=FALSE}

df2 = df[
    start_date >= '2019-03-23' & start_date <= '2019-04-05' & screen_first == 1,
    .(mean = mean(quality), sd = sd(quality)), 
    by = start_date
]
ggplot(df2, aes(x=start_date, y=mean)) + 
    geom_bar(stat="identity") +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2) +
    geom_vline(xintercept = as.numeric(as.Date('2019-03-30')) - .5, col = 'red', size = 1) +
    labs(x = NULL, y = 'Mean Quality', title = 'Mean Sleep Quality by Date (Screen Time First)') +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
    scale_x_date(breaks = df2$start_date, date_labels = '%b %d')
```

```{r quality_by_date_group2_fig, fig.cap='\\label{fig:quality_by_date_group2_fig} Mean sleep quality by date in group that received no screen time first', warning=FALSE}

df2 = df[
    start_date >= '2019-03-23' & start_date <= '2019-04-05' & screen_first == 0,
    .(mean = mean(quality), sd = sd(quality)), 
    by = start_date
]
ggplot(df2, aes(x=start_date, y=mean)) + 
    geom_bar(stat="identity") +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.2) +
    geom_vline(xintercept = as.numeric(as.Date('2019-03-30')) - .5, col = 'red', size = 1) +
    labs(x = NULL, y = 'Mean Quality', title = 'Mean Sleep Quality by Date (No Screen Time First)') +
    theme(axis.text.x = element_text(angle = 45, vjust = 0.5)) +
    scale_x_date(breaks = df2$start_date, date_labels = '%b %d')
```

Table \ref{tab:regressions} shows the results of three regressions models:  

1. $quality = \beta_0 + \beta_1 treat$
2. $quality = \beta_0 + \beta_1 treat + \beta_2 hours$
3. $quality = \beta_0 + \beta_1 treat + \beta_2 hours + \beta_3 id$

The covariates refer to treatment, hours slept, and unique indicators for each person in the study.  As expected, none of them show statistically significance of the treatment (screen-time before bed) affecting sleep quality.  The number of hours slept and the specific person has way more effect on sleep quality.

```{r}
m1 = df[treat %in% c(0,1), lm(quality ~ treat)]
m2 = df[treat %in% c(0,1), lm(quality ~ treat + hours)]
m3 = df[treat %in% c(0,1), lm(quality ~ treat + hours + id)]
```

```{r regressions, results='asis'}
stargazer(
    m1, m2, m3,
    header = F,
    title = '\\label{tab:regressions} Regressions',
    covariate.labels = c('Treatment (Screen-time)', 'Hours Slept', 'Subject ID')
)
```

## Conclusion

This experiment was too small to demonstrate any statistically significant treatment effect.  It would be interesting to repeat this test with a much larger population.